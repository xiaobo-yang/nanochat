# Queue Progress Snapshot (Run3 Done, Run4 Live)

- Snapshot time: 2026-02-24 16:41 HKT
- Branch: `yxb/moe-add-sft-claude`
- Experiment root: `/mnt/stepeval/yangxiaobo/cache/nanochat/tmp_exp/20260224_154946_moe-fp8-compile-ablation-v4`

## Queue Transition

From `launch.txt`:

- `moe_bf16_experts_dynamic_long` finished at `2026-02-24T16:39:47+08:00` with `status=0`
- `moe_fp8_experts_nocompile_long` started at `2026-02-24T16:39:49+08:00`

## Run3 Final Metrics

From `tmp_report/metrics/moe_fp8_ablation_summary.csv`:

- Run: `moe_bf16_experts_dynamic_long`
- Status: `completed`
- Steps: `360`
- Last loss: `3.522239`
- tail50 tok/sec: `1,038,953.14`
- tail50 bf16_mfu: `12.5716`
- tail50 dt_ms: `508.81`
- Peak memory: `21,101.70 MiB`
- Total training time: `3.65m`

## Run4 Live Status

From `moe_fp8_experts_nocompile_long.log` tail:

- Latest observed step: `00028/00360 (7.78%)`
- Live loss: `6.067822`
- Live tok/sec: `350,849`
- Live bf16_mfu: `4.25`
- Live dt: `1494.34ms`
- ETA at snapshot: `8.9m`

## GPU Utilization Sample

`nvidia-smi` at snapshot:

- GPU0 83% (26.5 GiB)
- GPU1 77% (26.5 GiB)
- GPU2 88% (26.5 GiB)
- GPU3 87% (26.5 GiB)
- GPU4 77% (26.5 GiB)
- GPU5 85% (26.5 GiB)
- GPU6 90% (26.5 GiB)
- GPU7 86% (26.5 GiB)

## Interim Observation

- Current no-compile FP8+experts run is markedly slower than dynamic-compile FP8+experts long run (`~0.35M` vs `~0.80M` tail throughput so far), as expected.
- Queue continuity is healthy: no GPU idle gap between Run3 completion and Run4 start.
