# Capacity Sweep Milestone: run1 done, run2 live

- Timestamp: 2026-02-24 17:36 +08:00
- Experiment: `/mnt/stepeval/yangxiaobo/cache/nanochat/tmp_exp/20260224_172715_moe-fp8-capacity-ablation-v1`
- Queue: `runs/moe_fp8_capacity_ablation_queue.sh`

## Queue Transition

- `17:35:07` run1 finished: `moe_fp8_experts_static_cap0p0` (status=0)
- `17:35:08` run2 started: `moe_fp8_experts_static_cap1p0`
- Transition was immediate and GPU tasks remained active.

## run1 Final Metrics (cap0.0, static, fp8 experts)

- steps: `280/280`
- total_time: `5.21 min`
- peak_mem: `22790.06 MiB`
- final loss: `3.718175`
- tail50 tok/sec: `468,782.94`
- tail50 dt: `1120.38 ms`
- tail50 bf16_mfu: `5.6722`

## run2 Live Early Metrics (cap1.0, static, fp8 experts)

- steps observed: `7`
- last step: `6/280`
- last loss: `8.023365`
- current tail(steps so far) tok/sec: `660,109.57`
- current tail(steps so far) dt: `2513.43 ms` (inflated by first cold step)

## Interpretation

- Compared with run1, run2 early tok/sec is materially higher, consistent with the hypothesis that fixed capacity can improve static-compile throughput by stabilizing expert token shapes.
- Current run2 tail metrics are still noisy because warmup includes `step0` compile overhead; final decision should use run2 tail50 near completion.

## Next Actions

1. Wait for run2 completion and extract true tail50 metrics.
2. Compare run2 vs run1 on `tok/sec`, `dt`, `mfu`, and `peak_mem`.
3. Continue to run3 (`cap1.25`) and dynamic/nocompile baselines for robustness.
